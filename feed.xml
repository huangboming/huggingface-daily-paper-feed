<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Hugging Face Daily Papers</title>
    <link>https://huggingface.co/papers</link>
    <description>Daily research papers curated by the Hugging Face community.</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Tue, 22 Jul 2025 00:02:31 +0000</lastBuildDate>
    <item>
      <title>RedOne: Revealing Domain-specific LLM Post-Training in Social Networking   Services</title>
      <link>https://arxiv.org/abs/2507.10605</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10605.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Fei Zhao, Chonggang Lu, Yue Wang, Zheyong Xie, Ziyan Liu, Haofu Qian, JianZhao Huang, Fangcheng Shi, Zijie Meng, Hongcheng Guo, Mingqian He, Xinze Lyu, Yiming Lu, Ziyang Xiang, Zheyu Ye, Chengqiang Lu, Zhe Xu, Yi Wu, Yao Hu, Yan Gao, Jun Fan, Xiaolong Jiang, Weiting Liu, Boyang Wang, Shaosheng Cao&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 4&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.10605</guid>
      <pubDate>Sun, 13 Jul 2025 02:22:59 +0000</pubDate>
    </item>
    <item>
      <title>The Devil behind the mask: An emergent safety vulnerability of Diffusion   LLMs</title>
      <link>https://arxiv.org/abs/2507.11097</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11097.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Zichen Wen, Jiashu Qu, Dongrui Liu, Zhiyuan Liu, Ruixi Wu, Yicun Yang, Xiangqi Jin, Haoyun Xu, Xuyang Liu, Weijia Li, Chaochao Lu, Jing Shao, Conghui He, Linfeng Zhang&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 39&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.11097</guid>
      <pubDate>Tue, 15 Jul 2025 08:44:46 +0000</pubDate>
    </item>
    <item>
      <title>Quantitative Risk Management in Volatile Markets with an Expectile-Based   Framework for the FTSE Index</title>
      <link>https://arxiv.org/abs/2507.13391</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13391.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Abiodun Finbarrs Oketunji&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 1&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; This research presents a framework for quantitative risk management in volatile markets, specifically focusing on expectile-based methodologies applied to the FTSE 100 index. Traditional risk measures such as Value-at-Risk (VaR) have demonstrated significant limitations during periods of market stress, as evidenced during the 2008 financial crisis and subsequent volatile periods. This study develops an advanced expectile-based framework that addresses the shortcomings of conventional quantile-based approaches by providing greater sensitivity to tail losses and improved stability in extreme market conditions. The research employs a dataset spanning two decades of FTSE 100 returns, incorporating periods of high volatility, market crashes, and recovery phases. Our methodology introduces novel mathematical formulations for expectile regression models, enhanced threshold determination techniques using time series analysis, and robust backtesting procedures. The empirical results demonstrate that expectile-based Value-at-Risk (EVaR) consistently outperforms traditional VaR measures across various confidence levels and market conditions. The framework exhibits superior performance during volatile periods, with reduced model risk and enhanced predictive accuracy. Furthermore, the study establishes practical implementation guidelines for financial institutions and provides evidence-based recommendations for regulatory compliance and portfolio management. The findings contribute significantly to the literature on financial risk management and offer practical tools for practitioners dealing with volatile market environments.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.13391</guid>
      <pubDate>Wed, 16 Jul 2025 08:24:14 +0000</pubDate>
    </item>
    <item>
      <title>Mitigating Object Hallucinations via Sentence-Level Early Intervention</title>
      <link>https://arxiv.org/abs/2507.12455</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12455.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shangpin Peng, Senqiao Yang, Li Jiang, Zhuotao Tian&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 3&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify a critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose **SENTINEL** (**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain pr**E**ference **L**earning), a framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively. Finally, we train models using a context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by over 90\% compared to the original model and outperforms the previous state-of-the-art method on both hallucination benchmarks and general capabilities benchmarks, demonstrating its superiority and generalization ability. The models, datasets, and code are available at https://github.com/pspdada/SENTINEL.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.12455</guid>
      <pubDate>Wed, 16 Jul 2025 17:55:43 +0000</pubDate>
    </item>
    <item>
      <title>Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal   Large Language Models</title>
      <link>https://arxiv.org/abs/2507.12566</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12566.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 9&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.12566</guid>
      <pubDate>Wed, 16 Jul 2025 18:31:23 +0000</pubDate>
    </item>
    <item>
      <title>Inverse Reinforcement Learning Meets Large Language Model Post-Training:   Basics, Advances, and Opportunities</title>
      <link>https://arxiv.org/abs/2507.13158</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13158.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Hao Sun, Mihaela van der Schaar&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 6&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.13158</guid>
      <pubDate>Thu, 17 Jul 2025 14:22:24 +0000</pubDate>
    </item>
    <item>
      <title>The Generative Energy Arena (GEA): Incorporating Energy Awareness in   Large Language Model (LLM) Human Evaluations</title>
      <link>https://arxiv.org/abs/2507.13302</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13302.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Carlos Arriaga, Gonzalo Mart√≠nez, Eneko Sendin, Javier Conde, Pedro Reviriego&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 1&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.13302</guid>
      <pubDate>Thu, 17 Jul 2025 17:11:14 +0000</pubDate>
    </item>
    <item>
      <title>A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges   in Russian Speech Generative Models</title>
      <link>https://arxiv.org/abs/2507.13563</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13563.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Kirill Borodin, Nikita Vasiliev, Vasiliy Kudryavtsev, Maxim Maslov, Mikhail Gorodnichev, Oleg Rogov, Grach Mkrtchian&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 41&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation. This paper introduces Balalaika, a novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations, including punctuation and stress markings. Experimental results show that models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks. We detail the dataset construction pipeline, annotation methodology, and results of comparative evaluations.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.13563</guid>
      <pubDate>Thu, 17 Jul 2025 22:41:40 +0000</pubDate>
    </item>
    <item>
      <title>CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models</title>
      <link>https://arxiv.org/abs/2507.13984</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13984.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Quang-Binh Nguyen, Minh Luu, Quang Nguyen, Anh Tran, Khoi Nguyen&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 7&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.13984</guid>
      <pubDate>Fri, 18 Jul 2025 14:45:48 +0000</pubDate>
    </item>
    <item>
      <title>OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder</title>
      <link>https://arxiv.org/abs/2507.14129</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14129.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shikhar Bharadwaj, Samuele Cornell, Kwanghee Choi, Satoru Fukayama, Hye-jin Shim, Soham Deshmukh, Shinji Watanabe&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 1&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multi-domain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding a billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs at https://shikhar-s.github.io/OpenBEATs&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.14129</guid>
      <pubDate>Fri, 18 Jul 2025 17:57:46 +0000</pubDate>
    </item>
    <item>
      <title>Franca: Nested Matryoshka Clustering for Scalable Visual Representation   Learning</title>
      <link>https://arxiv.org/abs/2507.14137</link>
      <description>&lt;p&gt;&lt;img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14137.png" alt="Paper thumbnail" style="max-width: 300px; height: auto;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Authors:&lt;/b&gt; Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano&lt;/p&gt;&lt;p&gt;&lt;b&gt;Upvotes:&lt;/b&gt; 14&lt;/p&gt;&lt;p&gt;&lt;b&gt;Summary:&lt;/b&gt; We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.&lt;/p&gt;</description>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.14137</guid>
      <pubDate>Fri, 18 Jul 2025 17:59:55 +0000</pubDate>
    </item>
  </channel>
</rss>
